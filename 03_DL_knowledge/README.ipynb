{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习知识总结\n",
    "先从伯禹的学习资料入手，边学习，边 [整理记录](https://github.com/shiyutang/Hands-on-deep-learning) +《\n",
    "[神经网络和深度学习](https://nndl.github.io/nndl-book.pdf)\n",
    "》补充；\n",
    "累了可以听听 [MIT 的6.S191](https://www.youtube.com/watch?v=JN6H4rQvwgY) refresh一下\n",
    "\n",
    "根据下列框架进行结构性的知识总结。\n",
    "\n",
    "1.浅层神经网络及模型基础\n",
    "\n",
    "* [1.1 线性回归](01_线性回归.ipynb)\n",
    "    * 定义\n",
    "    * 实施步骤\n",
    "    * 要点\n",
    "    * 最小二乘法（基于数据计算得到解析解的线性回归）\n",
    "        * 1. 矩阵推导出最小二乘解析解\n",
    "        * 2. 利用函数求导方式求得最小二乘估计解析解\n",
    "        * 3. 最小二乘估计的集合解释\n",
    "    * 主要函数\n",
    "    * 主要问题：\n",
    "        * 1.构建一个深度学习网络并训练需要哪些步骤\n",
    "        * 2.什么时候该用parameter.data\n",
    "\n",
    "* [1.2 分类模型和softmax](02_分类模型和Softmax.ipynb)\n",
    "    * 定义\n",
    "    * softmax的常数不变性\n",
    "    * softmax的优势\n",
    "    * 分类模型\n",
    "        * 定义\n",
    "        * 交叉熵函数\n",
    "\n",
    "* [1.3 多层感知机](03_多层感知机.ipynb)\n",
    "    * 定义\n",
    "    * 激活函数\n",
    "    * 主要问题：\n",
    "        * 如何选择不同激活函数\n",
    "\n",
    "* [1.4 模型选择（过拟合欠拟合的出现和解决)](04_模型选择（过拟合欠拟合出现和解决）.ipynb)\n",
    "    * 模型选择的方法\n",
    "        * 验证数据集进行验证\n",
    "        * k折验证法(数据少)\n",
    "    * 欠拟合和过拟合定义和影响因素\n",
    "    * 欠拟合和过拟合的解决方法\n",
    "        * 权重衰减和正则化\n",
    "        * dropout\n",
    "\n",
    "* [1.5 数值稳定性与模型初始化](05_数值稳定性与模型初始化.ipynb)\n",
    "    * 梯度消失和梯度爆炸\n",
    "    * 导致梯度消失和爆炸的原因\n",
    "    * 缓解梯度消失/爆炸：\n",
    "        * 神经元初始化\n",
    "        * 选择激活函数\n",
    "        * 批归一化\n",
    "\n",
    "2.卷积神经网络详解\n",
    "* [2.1 卷积神经网络](06_卷积神经网络.ipynb)\n",
    "    * 起源和特点\n",
    "    * 卷积神经网络组成\n",
    "    * 卷积层及其可选操作AM\n",
    "        *  空洞卷积  **todo**\n",
    "        *  感受野的计算  **todo**\n",
    "    * Pooling层\n",
    "    * 归一化层：\n",
    "        *  实例归一化 **todo**\n",
    "        *  批归一化\n",
    "            * 全连接层的批归一化\n",
    "            * 卷积层的批归一化\n",
    "            * 训练和预测的批归一化\n",
    "        *  组归一化 **todo**\n",
    "    * 损失函数  **todo**\n",
    "        *  交叉熵损失函数\n",
    "        *  L2损失\n",
    "        *  L1损失\n",
    "    * 卷积神经网络的整体结构\n",
    "    * 主要网络架构及其特点\n",
    "        * Lenet\n",
    "        * Alexnet\n",
    "        * VGG\n",
    "        * Network in Network（NIN）\n",
    "        * GoogLenet\n",
    "        * Resnet\n",
    "        * DenseNet\n",
    "\n",
    "* [3.注意力机制](08_注意力机制.ipynb)\n",
    "    * 1. 简介\n",
    "    * 2. SEnet (Squeeze-excitation network)\n",
    "    * 3. SKNET, CBAM 等\n",
    "\n",
    "* 4.循环神经网络  **todo**\n",
    "    * 基础\n",
    "    * GRU\n",
    "    * Lstm\n",
    "    * 深度循环神经网络\n",
    "    * 双向循环神经网络\n",
    "\n",
    "* 5.Transformer  **todo**\n",
    "\n",
    "* [6.优化](09_优化.ipynb)\n",
    "    * 深度学习优化：\n",
    "        * 深度学习优化和普通优化的差异\n",
    "        * 基于梯度的优化方法的挑战\n",
    "            * 山间缝隙（梯度反复震荡）\n",
    "            * 鞍点\n",
    "            * 梯度消失\n",
    "        * 基于梯度优化算法\n",
    "            * 证明：沿梯度反方向移动自变量可以减小函数值\n",
    "    * 凸优化\n",
    "        * 凸性\n",
    "        * 凸集合\n",
    "        * 凸函数的判定： Jensen 不等式\n",
    "        * 凸函数的性质\n",
    "            * 无局部最小\n",
    "            * 和凸集的关系：水平面截取定义域\n",
    "            * 二阶条件证明： 凸函数 $\\Longleftrightarrow$ 二阶导数大于0\n",
    "        * 如何优化带有限制条件的函数 \n",
    "    * 优化算法\n",
    "        * 优化方法：\n",
    "            * 解析方法\n",
    "            * 迭代方法\n",
    "                * 一阶方法\n",
    "                * 二阶方法\n",
    "        * 最速下降法:\n",
    "            * 凸函数下不同学习率的实验\n",
    "            * 非凸函数的实验\n",
    "            * 多维梯度下降\n",
    "        * 二阶方法：\n",
    "            * 优势\n",
    "            * 牛顿法\n",
    "            * 收敛性分析\n",
    "        * 共轭梯度法\n",
    "        * 随机梯度下降法\n",
    "            * 参数更新\n",
    "            * 抖动问题：\n",
    "                * 调整学习率\n",
    "                * 增大batch\n",
    "        * 小批量随机梯度下降法 (SGD)\n",
    "    * 高阶优化算法\n",
    "        * 病态问题：\n",
    "            * 公式定义：条件数\n",
    "            * 解决方法：\n",
    "                * 归一化统一量纲\n",
    "                * 平均历史梯度\n",
    "                * hessian矩阵预处理\n",
    "        * momentum（历史梯度滑动平均）\n",
    "            * 算法表达\n",
    "            * (指数加权平均)滑动平均\n",
    "                * 权值之和为（1-b^t）\n",
    "                * 指数加权平均约等于 $\\frac{1}{1-\\beta}$ 个历史结果的指数加权平均\n",
    "                * 由指数加权移动平均理解动量法\n",
    "        * AdaGrad: 累计梯度平方归一化       \n",
    "        * RMSProp：梯度平方滑动平均归一化\n",
    "        * AdaDelta：RMS基础上使用自变量变化量的指数滑动平均来替代学习率\n",
    "        * Adam：动量的滑动平均使用梯度平方滑动平均归一化当作自变量的梯度\n",
    "\n",
    "* 7.[模型微调](10_模型微调.ipynb)\n",
    "    * 模型微调的定义和方法\n",
    "    * 训练热狗分类任务\n",
    "        * 获取数据集\n",
    "        * 加载模型，设置微调层和优化器\n",
    "        * 使用/不使用模型微调的结果对比\n",
    "        * 总结\n",
    "\n",
    "\n",
    "* 8.GAN     **todo**\n",
    "    * basic\n",
    "    * DCGAN\n",
    "\n",
    "* 9.目标检测  **todo**\n",
    "\n",
    "* 10.语义分割  **todo**\n",
    "\n",
    "* 11.领域自适应  **todo**\n",
    "\n",
    "* 12.风格迁移  **todo**\n",
    "\n",
    "* 13.变化检测  **todo**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
