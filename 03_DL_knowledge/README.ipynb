{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习知识总结\n",
    "先从伯禹的学习资料入手，边学习，边 [整理记录](https://github.com/shiyutang/Hands-on-deep-learning) +《\n",
    "[神经网络和深度学习](https://nndl.github.io/nndl-book.pdf)\n",
    "》补充；\n",
    "累了可以听听 [MIT 的6.S191](https://www.youtube.com/watch?v=JN6H4rQvwgY) refresh一下\n",
    "\n",
    "根据下列框架进行结构性的知识总结。\n",
    "\n",
    "1.浅层神经网络及模型基础\n",
    "\n",
    "* [1.1 线性回归](01_线性回归.ipynb)\n",
    "    * 定义\n",
    "    * 实施步骤\n",
    "    * 要点\n",
    "    * 主要函数\n",
    "    * 主要问题：\n",
    "        * 1.构建一个深度学习网络并训练需要哪些步骤\n",
    "        * 2.什么时候该用parameter.data\n",
    "\n",
    "* [1.2 分类模型和softmax](02_分类模型和Softmax.ipynb)\n",
    "    * 定义\n",
    "    * softmax的性质\n",
    "    * softmax的优势\n",
    "    * 分类模型\n",
    "    * 交叉熵函数\n",
    "\n",
    "* [1.3 多层感知机](03_多层感知机.ipynb)\n",
    "    * 定义\n",
    "    * 激活函数\n",
    "    * 主要问题：\n",
    "        * 如何选择不同激活函数\n",
    "\n",
    "* [1.4 模型选择（过拟合欠拟合的出现和解决)](04_模型选择（过拟合欠拟合出现和解决）.ipynb)\n",
    "    * 模型选择的方法\n",
    "    * 欠拟合和过拟合定义和影响因素\n",
    "    * 欠拟合和过拟合的解决方法\n",
    "        * 权重衰减和正则化\n",
    "        * dropout\n",
    "\n",
    "* [1.5 数值稳定性与模型初始化](05_数值稳定性与模型初始化.ipynb)\n",
    "    * 梯度消失和梯度爆炸\n",
    "    * 导致梯度消失和爆炸的原因\n",
    "    * 神经原初始化\n",
    "\n",
    "2.卷积神经网络详解\n",
    "* [2.1 卷积神经网络](06_卷积神经网络.ipynb)\n",
    "    * 起源和特点\n",
    "    * 卷积神经网络组成\n",
    "    * 卷积层及其可选操作\n",
    "        *  空洞卷积  **todo**\n",
    "        *  感受野的计算  **todo**\n",
    "    * Pooling层\n",
    "    * 归一化层：\n",
    "        *  实例归一化 **todo**\n",
    "        *  批归一化\n",
    "        *  组归一化 **todo**\n",
    "    * 损失函数  **todo**\n",
    "        *  交叉熵损失函数\n",
    "        *  L2损失\n",
    "        *  L1损失\n",
    "    * 卷积神经网络的整体结构\n",
    "    * 主要网络架构及其特点\n",
    "        * Lenet\n",
    "        * Alexnet\n",
    "        * VGG\n",
    "        * Network in Network（NIN）\n",
    "        * GoogLenet\n",
    "        * Resnet\n",
    "        * DenseNet\n",
    "\n",
    "* 3.循环神经网络  **todo**\n",
    "    * 基础\n",
    "    * GRU\n",
    "    * Lstm\n",
    "    * 深度循环神经网络\n",
    "    * 双向循环神经网络\n",
    "\n",
    "* [4.注意力机制](08_注意力机制.ipynb)\n",
    "    * 简介\n",
    "    * SEnet (Squeeze-excitation network)\n",
    "\n",
    "* 5.Transformer  **todo**\n",
    "\n",
    "* [6.优化](09_优化.ipynb)\n",
    "    * 深度学习优化：\n",
    "        * 深度学习优化和普通优化的差异\n",
    "        * 基于梯度的优化方法的挑战\n",
    "    * 凸优化\n",
    "        * 凸性\n",
    "        * 凸函数的性质和 Jensen 不等式\n",
    "        * 如何优化带有限制条件的函数 \n",
    "    * 优化算法\n",
    "        * 牛顿法\n",
    "        * 共轭梯度法\n",
    "        * 随机梯度下降法\n",
    "        * 小批量随机梯度下降法 (SGD)\n",
    "    * 高阶优化算法\n",
    "        * momentum\n",
    "        * AdaGrad       \n",
    "        * RMSProp\n",
    "        * AdaDelta\n",
    "        * Adam\n",
    "\n",
    "* 7.[模型微调](10_模型微调.ipynb)\n",
    "    * 模型微调的定义和方法\n",
    "    * 训练热狗分类任务\n",
    "        * 获取数据集\n",
    "        * 加载模型，设置微调层和优化器\n",
    "        * 使用/不使用模型微调的结果对比\n",
    "        * 总结\n",
    "\n",
    "\n",
    "* 8.GAN     **todo**\n",
    "    * basic\n",
    "    * DCGAN\n",
    "\n",
    "* 9.目标检测  **todo**\n",
    "\n",
    "* 10.语义分割  **todo**\n",
    "\n",
    "* 11.领域自适应  **todo**\n",
    "\n",
    "* 12.风格迁移  **todo**\n",
    "\n",
    "* 13.变化检测  **todo**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}