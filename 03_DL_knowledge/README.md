# 深度学习知识总结
先从伯禹的学习资料入手，边学习，边 [整理记录](https://github.com/shiyutang/Hands-on-deep-learning) +《
[神经网络和深度学习](https://nndl.github.io/nndl-book.pdf)
》补充；
累了可以听听 [MIT 的6.S191](https://www.youtube.com/watch?v=JN6H4rQvwgY) refresh一下

根据下列框架进行结构性的知识总结。

1.浅层神经网络及模型基础

* [1.1 线性回归](01_线性回归.ipynb)
    * 定义
    * 实施步骤
    * 要点
    * 最小二乘法（基于数据计算得到解析解的线性回归）
        * 1. 矩阵推导出最小二乘解析解
        * 2. 利用函数求导方式求得最小二乘估计解析解
        * 3. 最小二乘估计的集合解释
    * 主要函数
    * 主要问题：
        * 1.构建一个深度学习网络并训练需要哪些步骤
        * 2.什么时候该用parameter.data

* [1.2 分类模型和softmax](02_分类模型和Softmax.ipynb)
    * 定义
    * softmax的常数不变性
    * softmax的优势
    * 分类模型
        * 定义
        * 交叉熵函数

* [1.3 多层感知机](03_多层感知机.ipynb)
    * 定义
    * 激活函数
    * 主要问题：
        * 如何选择不同激活函数

* [1.4 模型选择（过拟合欠拟合的出现和解决)](04_模型选择（过拟合欠拟合出现和解决）.ipynb)
    * 模型选择的方法
        * 验证数据集进行验证
        * k折验证法(数据少)
    * 欠拟合和过拟合定义和影响因素
    * 欠拟合和过拟合的解决方法
        * 权重衰减和正则化
        * dropout

* [1.5 数值稳定性与模型初始化](05_数值稳定性与模型初始化.ipynb)
    * 梯度消失和梯度爆炸
    * 导致梯度消失和爆炸的原因
    * 缓解梯度消失/爆炸：
        * 神经元初始化
        * 选择激活函数
        * 批归一化

2.卷积神经网络详解
* [2.1 卷积神经网络](06_卷积神经网络.ipynb)
    * 起源和特点
    * 卷积神经网络组成
    * 卷积层及其可选操作AM
        *  空洞卷积  **todo**
        *  感受野的计算  **todo**
    * Pooling层
    * 归一化层：
        *  实例归一化 **todo**
        *  批归一化
            * 全连接层的批归一化
            * 卷积层的批归一化
            * 训练和预测的批归一化
        *  组归一化 **todo**
    * 损失函数  **todo**
        *  交叉熵损失函数
        *  L2损失
        *  L1损失
    * 卷积神经网络的整体结构
    * 主要网络架构及其特点
        * Lenet
        * Alexnet
        * VGG
        * Network in Network（NIN）
        * GoogLenet
        * Resnet
        * DenseNet

* [3.注意力机制](08_注意力机制.ipynb)
    * 1. 简介
    * 2. SEnet (Squeeze-excitation network)
    * 3. SKNET, CBAM 等

* 4.循环神经网络  **todo**
    * 基础
    * GRU
    * Lstm
    * 深度循环神经网络
    * 双向循环神经网络

* 5.Transformer  **todo**

* [6.优化](09_优化.ipynb)
    * 深度学习优化：
        * 深度学习优化和普通优化的差异
        * 基于梯度的优化方法的挑战
            * 山间缝隙（梯度反复震荡）
            * 鞍点
            * 梯度消失
        * 基于梯度优化算法
            * 证明：沿梯度反方向移动自变量可以减小函数值
    * 凸优化
        * 凸性
        * 凸集合
        * 凸函数的判定： Jensen 不等式
        * 凸函数的性质
            * 无局部最小
            * 和凸集的关系：水平面截取定义域
            * 二阶条件证明： 凸函数 $\Longleftrightarrow$ 二阶导数大于0
        * 如何优化带有限制条件的函数 
    * 优化算法
        * 优化方法：
            * 解析方法
            * 迭代方法
                * 一阶方法
                * 二阶方法
        * 最速下降法:
            * 凸函数下不同学习率的实验
            * 非凸函数的实验
            * 多维梯度下降
        * 二阶方法：
            * 优势
            * 牛顿法
            * 收敛性分析
        * 共轭梯度法
        * 随机梯度下降法
            * 参数更新
            * 抖动问题：
                * 调整学习率
                * 增大batch
        * 小批量随机梯度下降法 (SGD)
    * 高阶优化算法
        * 病态问题：
            * 公式定义：条件数
            * 解决方法：
                * 归一化统一量纲
                * 平均历史梯度
                * hessian矩阵预处理
        * momentum（历史梯度滑动平均）
            * 算法表达
            * (指数加权平均)滑动平均
                * 权值之和为（1-b^t）
                * 指数加权平均约等于 $\frac{1}{1-\beta}$ 个历史结果的指数加权平均
                * 由指数加权移动平均理解动量法
        * AdaGrad: 累计梯度平方归一化       
        * RMSProp：梯度平方滑动平均归一化
        * AdaDelta：RMS基础上使用自变量变化量的指数滑动平均来替代学习率
        * Adam：动量的滑动平均使用梯度平方滑动平均归一化当作自变量的梯度

* 7.[模型微调](10_模型微调.ipynb)
    * 模型微调的定义和方法
    * 训练热狗分类任务
        * 获取数据集
        * 加载模型，设置微调层和优化器
        * 使用/不使用模型微调的结果对比
        * 总结


* 8.GAN     **todo**
    * basic
    * DCGAN

* 9.目标检测  **todo**

* 10.语义分割  **todo**

* 11.领域自适应  **todo**

* 12.风格迁移  **todo**

* 13.变化检测  **todo**