{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# DL-Prep\n",
    "**深度学习基础知识，算法，面试题**\n",
    "**由 UESTC DL 学习小组开发和维护**\n",
    "\n",
    "1. [数学相关(Math)](01_Math/README.ipynb)\n",
    "2. [机器学习基础相关(ML)](02_ML_knowledge/README.ipynb)\n",
    "3. [深度学习相关(DL)](03_DL_knowledge/README.ipynb)\n",
    "4. [基础算法题相关(AL)](04_Algorithms/README.ipynb)\n",
    "5. [语言相关(LA)](05_Language/README.ipynb)\n",
    "6. [深度学习框架(DF)](06_DL_framework/README.ipynb)\n",
    "6. [面试题(IQ)](07_Interview_Questions/README.ipynb)\n",
    "\n",
    "\n",
    "## 维护笔记\n",
    "1. 当需要更改前，增加一个新的 branch，确认自己的更改好之后，再和 master 合并\n",
    "1. 当需要添加新知识时，可以对照目录，插入到对应的位置，并按照格式增加目录和在相应位置增加内容\n",
    "2. 插入图片时，先将图片放入 Pics 文件夹，随后按照相对路径插入\n",
    "3. 插入了目录，但是又没有时间写内容？ 可以加个 **todo** 之后补全\n",
    "3. 大纲和格式可能有错漏，欢迎互相指正修改~\n",
    "\n",
    "### 增加了 Jupyter Notebook之后\n",
    "我们将在 README.md 的基础上增加 Jupyter Notebook， 这样，我们可以方便在本地查看公式，并运行代码。MD则在 Github 上可以提供大纲的预览。\n",
    "建议的方式是：\n",
    "- 查看：将 Github 的仓库拉取到本地，并在本地 launch Jupyter Server 进行查看，查看时先打开第一层级的 README.ipynb 进行索引\n",
    "- 修改：新开 branch 后在 Pycharm / Jupyter Notebook 等编辑器中编辑，编辑 Jupyter Notebook 的同时，我们需要在MD里面进行目录的同步。并用【查看】的方式预览编辑效果\n",
    "- 合并：本地修改好了之后，使用 Git commit， Git push， 再在本地或者云端merge\n",
    "\n",
    "原则是：Jupyter Notebook 负责内容，因此 MarkDown 中有的内容，Jupyter Notebook 中一定会有； 同时 Readme.md 提供预览，因此需要有大纲和相关说明。\n",
    "先行编辑 Jupyter Notebook，之后在 MarkDown 中相应更新。\n",
    "\n",
    "### 如何添加 Anchor ？\n",
    "Anchor 帮助我们快速跳转到需要的内容，那么如何添加 anchor 呢？\n",
    "详见 [添加Anchor](Utils/AddAnchor.ipynb)\n",
    "\n",
    "## 其他\n",
    "欢迎大家 fork, pull request, star 这个项目\n",
    "大部分都是手打，根据资料整理，如有问题，欢迎 fork 之后 pull request 或者 issue 中提出，我们会尽可能及时回复\n",
    "\n",
    "## 主要内容\n",
    "\n",
    "### 1. 数学知识：矩阵理论，概率论等\n",
    "* 2.线性代数\n",
    "    * 2.1 标量，向量，矩阵和张量\n",
    "    * 2.2 矩阵和向量相乘：\n",
    "    * 2.3 单位矩阵和逆矩阵：\n",
    "    * 2.4 线性相关和生成子空间：\n",
    "    * 2.5 范数:衡量向量大小的函数\n",
    "\n",
    "* 3.概率和信息论\n",
    "\n",
    "* 4.数值计算\n",
    "    * 4.1 上溢和下溢\n",
    "    * 4.2 病态条件\n",
    "    * 4.3基于梯度的优化方法\n",
    "        * 4.3.1 雅可比和海森矩阵\n",
    "\n",
    "### 2.#  机器学习知识总结\n",
    "李航机器学习整理+ [this repo](https://github.com/shiyutang/MachineLearning)+ [shuhuai 的白板推导视频](https://www.bilibili.com/video/BV1aE411o7qd?p=2)\n",
    "\n",
    "* [1. 频率派和贝叶斯派](#1.频率派和贝叶斯派)\n",
    "    * 频率派\n",
    "    * 贝叶斯派\n",
    "\n",
    "* [2. 聚类](#2.聚类) \n",
    "    * 聚集派\n",
    "    * 连接派\n",
    "\n",
    "* [3. 降维](#3.降维)\n",
    "    * PCA\n",
    "    * SVD\n",
    "\n",
    "\n",
    "\n",
    "### 3. 深度学习知识总结\n",
    "先从伯禹的学习资料入手，边学习，边 [整理记录](https://github.com/shiyutang/Hands-on-deep-learning) +《\n",
    "[神经网络和深度学习](https://nndl.github.io/nndl-book.pdf)\n",
    "》补充；\n",
    "累了可以听听 [MIT 的6.S191](https://www.youtube.com/watch?v=JN6H4rQvwgY) refresh一下\n",
    "\n",
    "根据下列框架进行结构性的知识总结。\n",
    "\n",
    "1.浅层神经网络及模型基础\n",
    "\n",
    "* [1.1 线性回归](01_线性回归.ipynb)\n",
    "    * 定义\n",
    "    * 实施步骤\n",
    "    * 要点\n",
    "    * 最小二乘法（基于数据计算得到解析解的线性回归）\n",
    "        * 1. 矩阵推导出最小二乘解析解\n",
    "        * 2. 利用函数求导方式求得最小二乘估计解析解\n",
    "        * 3. 最小二乘估计的集合解释\n",
    "    * 主要函数\n",
    "    * 主要问题：\n",
    "        * 1.构建一个深度学习网络并训练需要哪些步骤\n",
    "        * 2.什么时候该用parameter.data\n",
    "\n",
    "* [1.2 分类模型和softmax](02_分类模型和Softmax.ipynb)\n",
    "    * 定义\n",
    "    * softmax的常数不变性\n",
    "    * softmax的优势\n",
    "    * 分类模型\n",
    "        * 定义\n",
    "        * 交叉熵函数\n",
    "\n",
    "* [1.3 多层感知机](03_多层感知机.ipynb)\n",
    "    * 定义\n",
    "    * 激活函数\n",
    "    * 主要问题：\n",
    "        * 如何选择不同激活函数\n",
    "\n",
    "* [1.4 模型选择（过拟合欠拟合的出现和解决)](04_模型选择（过拟合欠拟合出现和解决）.ipynb)\n",
    "    * 模型选择的方法\n",
    "        * 验证数据集进行验证\n",
    "        * k折验证法(数据少)\n",
    "    * 欠拟合和过拟合定义和影响因素\n",
    "    * 欠拟合和过拟合的解决方法\n",
    "        * 权重衰减和正则化\n",
    "        * dropout\n",
    "\n",
    "* [1.5 数值稳定性与模型初始化](05_数值稳定性与模型初始化.ipynb)\n",
    "    * 梯度消失和梯度爆炸\n",
    "    * 导致梯度消失和爆炸的原因\n",
    "    * 缓解梯度消失/爆炸：\n",
    "        * 神经元初始化\n",
    "        * 选择激活函数\n",
    "        * 批归一化\n",
    "\n",
    "2.卷积神经网络详解\n",
    "* [2.1 卷积神经网络](06_卷积神经网络.ipynb)\n",
    "    * 起源和特点\n",
    "    * 卷积神经网络组成\n",
    "    * 卷积层及其可选操作AM\n",
    "        *  空洞卷积  **todo**\n",
    "        *  感受野的计算  **todo**\n",
    "    * Pooling层\n",
    "    * 归一化层：\n",
    "        *  实例归一化 **todo**\n",
    "        *  批归一化\n",
    "            * 全连接层的批归一化\n",
    "            * 卷积层的批归一化\n",
    "            * 训练和预测的批归一化\n",
    "        *  组归一化 **todo**\n",
    "    * 损失函数  **todo**\n",
    "        *  交叉熵损失函数\n",
    "        *  L2损失\n",
    "        *  L1损失\n",
    "    * 卷积神经网络的整体结构\n",
    "    * 主要网络架构及其特点\n",
    "        * Lenet\n",
    "        * Alexnet\n",
    "        * VGG\n",
    "        * Network in Network（NIN）\n",
    "        * GoogLenet\n",
    "        * Resnet\n",
    "        * DenseNet\n",
    "\n",
    "* [3.注意力机制](08_注意力机制.ipynb)\n",
    "    * 1. 简介\n",
    "    * 2. SEnet (Squeeze-excitation network)\n",
    "    * 3. SKNET, CBAM 等\n",
    "\n",
    "* 4.循环神经网络  **todo**\n",
    "    * 基础\n",
    "    * GRU\n",
    "    * Lstm\n",
    "    * 深度循环神经网络\n",
    "    * 双向循环神经网络\n",
    "\n",
    "* 5.Transformer  **todo**\n",
    "\n",
    "* [6.优化](09_优化.ipynb)\n",
    "    * 深度学习优化：\n",
    "        * 深度学习优化和普通优化的差异\n",
    "        * 基于梯度的优化方法的挑战\n",
    "            * 山间缝隙（梯度反复震荡）\n",
    "            * 鞍点\n",
    "            * 梯度消失\n",
    "        * 基于梯度优化算法\n",
    "            * 证明：沿梯度反方向移动自变量可以减小函数值\n",
    "    * 凸优化\n",
    "        * 凸性\n",
    "        * 凸集合\n",
    "        * 凸函数的判定： Jensen 不等式\n",
    "        * 凸函数的性质\n",
    "            * 无局部最小\n",
    "            * 和凸集的关系：水平面截取定义域\n",
    "            * 二阶条件证明： 凸函数 $\\Longleftrightarrow$ 二阶导数大于0\n",
    "        * 如何优化带有限制条件的函数 \n",
    "    * 优化算法\n",
    "        * 优化方法：\n",
    "            * 解析方法\n",
    "            * 迭代方法\n",
    "                * 一阶方法\n",
    "                * 二阶方法\n",
    "        * 最速下降法:\n",
    "            * 凸函数下不同学习率的实验\n",
    "            * 非凸函数的实验\n",
    "            * 多维梯度下降\n",
    "        * 二阶方法：\n",
    "            * 优势\n",
    "            * 牛顿法\n",
    "            * 收敛性分析\n",
    "        * 共轭梯度法\n",
    "        * 随机梯度下降法\n",
    "            * 参数更新\n",
    "            * 抖动问题：\n",
    "                * 调整学习率\n",
    "                * 增大batch\n",
    "        * 小批量随机梯度下降法 (SGD)\n",
    "    * 高阶优化算法\n",
    "        * 病态问题：\n",
    "            * 公式定义：条件数\n",
    "            * 解决方法：\n",
    "                * 归一化统一量纲\n",
    "                * 平均历史梯度\n",
    "                * hessian矩阵预处理\n",
    "        * momentum（历史梯度滑动平均）\n",
    "            * 算法表达\n",
    "            * (指数加权平均)滑动平均\n",
    "                * 权值之和为（1-b^t）\n",
    "                * 指数加权平均约等于 $\\frac{1}{1-\\beta}$ 个历史结果的指数加权平均\n",
    "                * 由指数加权移动平均理解动量法\n",
    "        * AdaGrad: 累计梯度平方归一化       \n",
    "        * RMSProp：梯度平方滑动平均归一化\n",
    "        * AdaDelta：RMS基础上使用自变量变化量的指数滑动平均来替代学习率\n",
    "        * Adam：动量的滑动平均使用梯度平方滑动平均归一化当作自变量的梯度\n",
    "\n",
    "* 7.[模型微调](10_模型微调.ipynb)\n",
    "    * 模型微调的定义和方法\n",
    "    * 训练热狗分类任务\n",
    "        * 获取数据集\n",
    "        * 加载模型，设置微调层和优化器\n",
    "        * 使用/不使用模型微调的结果对比\n",
    "        * 总结\n",
    "\n",
    "\n",
    "* 8.GAN     **todo**\n",
    "    * basic\n",
    "    * DCGAN\n",
    "\n",
    "* 9.目标检测  **todo**\n",
    "\n",
    "* 10.语义分割  **todo**\n",
    "\n",
    "* 11.领域自适应  **todo**\n",
    "\n",
    "* 12.风格迁移  **todo**\n",
    "\n",
    "* 13.变化检测  **todo**\n",
    "\n",
    "### 4. 主要算法总结\n",
    "根据斯坦福 CS97SI 课程整理相关知识，并辅助以leetcode中有疑惑的算法题解析进行讨论\n",
    "\n",
    "根据CS 97SI 总结的笔记，以及 python 语法笔记，在[这里](https://www.notion.so/shiyu00daisy/Xtreme-0f9c9b3264ea4126b02dc89224d6a524) \n",
    "\n",
    "### 5. Python等计算机语言\n",
    "这部分主要有一个大概框架来汇总语法糖和语言中常见的机制内部实现等\n",
    "\n",
    "### 6. 深度学习框架\n",
    "总结深度学习框架：pytorch，tensorflow等中的语法和机制\n",
    "\n",
    "根据[深度学习框架pytorch 入门与实践](https://github.com/chenyuntc/pytorch-book) 的结构总结+[网页](https://pytorch.apachecn.org/docs/1.2/intermediate/model_parallel_tutorial.html)\n",
    "\n",
    "### 7. 面试题\n",
    "将见过的面试题和自己想到查到的回答\n",
    "\n",
    "* [1. 训练加速](#1.训练加速)\n",
    "    * 数据并行\n",
    "        * Pytorch 的 DataParallel 流程图\n",
    "    * 数据并行(nn.distributed) \\* [todo]\n",
    "    * 模型并行\n",
    "    * pytorch的dataparallel\n",
    "\n",
    "* [2. 目标检测](#2.目标检测) \n",
    "    * 1.算法有哪些？他们的对比？\n",
    "        * RCNN：\n",
    "        * fast RCNN\n",
    "        * faster RCNN\n",
    "        * YOLO\n",
    "            * YOLOV2\n",
    "            * YOLOV3\n",
    "        * SSD\n",
    "    * 2.简述一下YOLOv2的原理，v1和v2有什么区别？\n",
    "    * 3.非极大抑制是什么，有什么作用？\n",
    "    * 4.如何计算 Anchor 和真实 bbox 之间的 mIOU，以及如何在此基础上进行 NMS 算法？\n",
    "    * 5.nms的发展（greedy-nms，soft-nms，fast-nms，matrix-nms） \\* [todo]\n",
    "    * 6.YOLOv2为什么将输入尺寸从448降到416\n",
    "    * 7.YOLOv2对于anchor的使用与faster-rcnn有何不同?\n",
    "    * 8.YOLOv2,v3一个anchor可以对应几个GT？SSD呢？RCNN系列呢？\n",
    "    * 9.YOLOv3对于v2做了怎样的改进？\n",
    "    * 10.YOLOv2与v3筛选正负样本的方式类似，具体是怎样进行的？这种操作解决了什么问题？\n",
    "    * 11.YOLOv3的多尺度输出结构与FPN有何不同？\n",
    "    * 12.YOLOv2,v3的anchor聚类如何做？指标是什么？\n",
    "    * 13.FPN的多尺度输出结构与SSD的多尺度输出结构哪个效果更好\n",
    "    * 14.faster-rcnn在撒anchor的时候，是如何把特征图坐标映射到图像上的？\n",
    "    * 15.faster-rcnn的OHEM与ssd的OHEM有何不同\n",
    "    * 16.roi pooling与roi align的具体操作\n",
    "    * 17.retinanet解决了以往one-stage检测器的什么问题\n",
    "    * 18.Focal loss一定有效吗？为什么？试举出一个例子\n",
    "    * 19.介绍cascade-RCNN和DCN模块。Cascade-rcnn解决了什么问题？cascade-RCNN一般选用几个阶段？\n",
    "    * 20.anchor-free的方式大概分为哪两种？各有什么特点？\n",
    "    * 21.coco的mAP的计算公式\n",
    "    * 22.为什么 Faster RCNN 相较 YOLO v2 更慢？\n",
    "    * 23.为什么 Faster RCNN 预测准确度相比于 一阶段网络更高？\n",
    "    * 24.介绍一下 RPN 的结构？\n",
    "    * 25.YOLO 和 SSD 的区别？\n",
    "\n",
    "* [3. 循环卷积神经网络](#3.循环卷积神经网络) \n",
    "    * 1.LSTM为什么会导致梯度爆炸？要如何解决？\n",
    "\n",
    "* [4. 语义分割](#4.语义分割) \n",
    "    * 1.主要语义分割的算法有哪些，他们有什么区别？\n",
    "        * 1. FCN(全卷积网络)\n",
    "        * 2. Unet\n",
    "        * 3. SegNet\n",
    "        * 4. Deeplab v3+\n",
    "        * 5. PSPNet(金字塔场景分割网络) \n",
    "    * 2.感受野会受到什么因素的影响？怎么影响？\n",
    "    * 3.遥感图像语义分割和普通图像的语义分割有什么区别？\n",
    "    * 4.语义分割中的样本不均衡介绍一下？\n",
    "    * 5.双线性插值，转置卷积和反卷积的区别与联系 \\* [todo]\n",
    "    * 6.介绍语义分割、实例分割和全景分割 \\* [todo]\n",
    "    * 7.后处理方法：CRF \\* [todo]\n",
    "    * 8.介绍空洞卷积以及DeepLabv3中的ASPP模块 \\* [todo]\n",
    "\n",
    "\n",
    "* [5. Backbone](#5.Backbone) \n",
    "    * 1.resnet中的恒等快捷连接在前向传播和反向传播都有什么作用？\n",
    "    * 2.ResNet和ResNeXt的区别 \\* [todo]\n",
    "    * 3.Inception中的deep supervision有什么作用 \\* [todo]  \n",
    "    * 4.resnet的shortcut结构有什么缺点?如何改进? \\* [todo]    \n",
    "    * 5.resnet的post activation有什么作用? \\* [todo]  \n",
    "    * 6.shufflenet的shuffle操作如何进行? \\* [todo]  \n",
    "    * 7.比较一下 Inception v1， v2，v3？\n",
    "\n",
    "* [6. 卷积神经网络基础](#6.卷积神经网络基础)\n",
    "    * 1.Batch Normalization 在卷积神经网络中的作用是什么?\n",
    "    * 2.1* 1卷积的作用是什么？\n",
    "    * 2.1. 深度可分离卷积介绍一下?\n",
    "    * 3.两层较小的卷积核和一个较大的卷积核比较，各有什么缺点和优点？\n",
    "    * 4.不同激活函数有什么区别？\n",
    "    * 5.卷积层输出大小的计算?\n",
    "    * 5.1.卷积层中计算量的计算：\n",
    "    * 6.dropout层为什么可以促进正则化？pytorch中dropout在训练与测试时如何使用？\n",
    "    * 7.平方误差损失函数和交叉熵损失函数分别适用于什么场景？\n",
    "    * 8.梯度消失/爆炸的原因?\n",
    "    * 9.损失降不下来怎么办？\n",
    "    * 10.weight decay vs L2 正则项\n",
    "    * 11.avarage-pooling与max-pooling的区别与联系？它们的梯度反传如何进行？\n",
    "    * 12.各种normalization层了解多少？(包括SyncBN)\n",
    "    * 13.为什么学习率的设置要与batchsize成线型关系\n",
    "    * 14.ReLU有哪些改进的方式\n",
    "    * 15.神经网络中参数中的偏置bias有什么作用\n",
    "    * 16.caffe的im2col是怎么操作的？ \\* [todo]\n",
    "    * 17.转置卷积和反卷积的区别？\n",
    "    * 18.什么是空洞卷积，它的感受野如何计算？ 给定 dialation rate 为 Drate 的卷积核，他的输出和输入大小的对应关系又是什么？\n",
    "\n",
    "\n",
    "* [7. 神经网络训练场景问题](#7.神经网络训练场景问题)\n",
    "    * 1.怎么判断过拟合，怎么处理？\n",
    "    * 2.怎么解决图像语义分割中的样本不均衡问题?\n",
    "\n",
    "* [8. Python](#8.Python)\n",
    "    * 1.装饰器是什么，有什么作用？\n",
    "    * 2.迭代器\n",
    "    * 3.生成器\n",
    "    * 4.深拷贝与浅拷贝的区别 \\* [todo]\n",
    "    * 5.Python中is和==的区别\n",
    "    * 6.解释with语句\n",
    "    * 7.什么是面向对象？面向过程和面向对象的区别？\n",
    "    \n",
    "* [9. 机器学习](#9.机器学习)\n",
    "    * 1.讲讲支持向量机，间隔，对偶，核技巧，如何将分类问题转化成最优化问题的（手推公式）？\n",
    "    * 2.手写逻辑回归的损失函数 \\* [todo]\n",
    "    * 3.简单介绍一下集成学习？\n",
    "        * BAGGING\n",
    "        * BOOSTING\n",
    "        * GB\n",
    "        * STACKING\n",
    "\n",
    "* [10.传统图像处理](#10.传统图像处理)\n",
    "    * 1.直方图均衡  \n",
    "    * 2.直方图匹配  \n",
    "    * 3.SIFT的特点  \n",
    "    * 4.刚体变换，仿射变换，透视变换（投影变换）  \n",
    "    * 5.索贝尔算子长什么样?为什么长这样?\n",
    "    * 6.拉普拉斯边算子?  \n",
    "    * 7.图像亮度、对比度、饱和度 \n",
    "    * 8.高斯金字塔与拉普拉斯金字塔\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
